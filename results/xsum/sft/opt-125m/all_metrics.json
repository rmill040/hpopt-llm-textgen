[{"train_loss": 2.79734375, "perplexity": 14.694892728788941, "validation_loss": 2.6875, "epoch": 1, "step": 100}, {"train_loss": 2.772734375, "perplexity": 14.242778409807016, "validation_loss": 2.65625, "epoch": 1, "step": 200}, {"train_loss": 2.75671875, "perplexity": 14.242778409807016, "validation_loss": 2.65625, "epoch": 1, "step": 300}, {"train_loss": 2.7460546875, "perplexity": 14.021964597512564, "validation_loss": 2.640625, "epoch": 1, "step": 400}, {"train_loss": 2.73809375, "perplexity": 13.804574186067095, "validation_loss": 2.625, "epoch": 1, "step": 500}, {"train_loss": 2.7280989583333333, "perplexity": 13.804574186067095, "validation_loss": 2.625, "epoch": 1, "step": 600}, {"train_loss": 2.7200892857142858, "perplexity": 13.804574186067095, "validation_loss": 2.625, "epoch": 1, "step": 700}, {"train_loss": 2.71193359375, "perplexity": 13.59055410055989, "validation_loss": 2.609375, "epoch": 2, "step": 800}, {"train_loss": 2.707222222222222, "perplexity": 13.59055410055989, "validation_loss": 2.609375, "epoch": 2, "step": 900}, {"train_loss": 2.703453125, "perplexity": 13.59055410055989, "validation_loss": 2.609375, "epoch": 2, "step": 1000}, {"train_loss": 2.7, "perplexity": 13.59055410055989, "validation_loss": 2.609375, "epoch": 2, "step": 1100}, {"train_loss": 2.6958984375, "perplexity": 13.59055410055989, "validation_loss": 2.609375, "epoch": 2, "step": 1200}, {"train_loss": 2.693245192307692, "perplexity": 13.59055410055989, "validation_loss": 2.609375, "epoch": 2, "step": 1300}, {"train_loss": 2.6907700892857145, "perplexity": 13.59055410055989, "validation_loss": 2.609375, "epoch": 2, "step": 1400}, {"train_loss": 2.687895833333333, "perplexity": 13.379852088930456, "validation_loss": 2.59375, "epoch": 3, "step": 1500}, {"train_loss": 2.684755859375, "perplexity": 13.379852088930456, "validation_loss": 2.59375, "epoch": 3, "step": 1600}, {"train_loss": 2.682610294117647, "perplexity": 13.379852088930456, "validation_loss": 2.59375, "epoch": 3, "step": 1700}, {"train_loss": 2.6805121527777778, "perplexity": 13.379852088930456, "validation_loss": 2.59375, "epoch": 3, "step": 1800}, {"train_loss": 2.6780263157894737, "perplexity": 13.379852088930456, "validation_loss": 2.59375, "epoch": 3, "step": 1900}, {"train_loss": 2.67621875, "perplexity": 13.379852088930456, "validation_loss": 2.59375, "epoch": 3, "step": 2000}, {"train_loss": 2.6747544642857144, "perplexity": 13.379852088930456, "validation_loss": 2.59375, "epoch": 3, "step": 2100}]